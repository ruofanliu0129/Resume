---
title: "RUOFAN LIU - About Me"
description: "Self Introduction by RUOFAN LIU."
permalink: /
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Ph.D. student (3rd year) at the [Deparment of Computer Science](https://educ.titech.ac.jp/cs/eng/), [Institute of Science Tokyo](https://www.isct.ac.jp/en), supervised by Prof. [Hideki Koike](https://scholar.google.com/citations?hl=en&user=Ih8cJXQAAAAJ), currently a visiting researcher at [Stanford Computational Imaging Lab](https://www.computationalimaging.org/), [Stanford University](https://www.stanford.edu/), supervised by Prof. [Gordon Wetzstein](https://scholar.google.com/citations?user=VOf45S0AAAAJ&hl=en), and a research fellow at [Sony Computer Science Laboratories](https://www.sonycsl.co.jp/), supervised by Prof. [Shinichi Furuya](https://scholar.google.com/citations?hl=en&user=IphDyJcAAAAJ).

I obtained my M.E. in Computer Science at the Institute of Science Tokyo with Prof. Hideki Koike, and obtained my B.E. in Computer Science and Technology at Shanghai Jiao Tong University with Prof. [Baoliang Lu](https://scholar.google.com/citations?hl=en&user=709il6EAAAAJ). 

My research lies at the intersection of <strong>computational sensing, multimodal generative methods, and human-AI interaction</strong>. My ultimate research goal is to create a next-generation deep learning framework and a generative interaction model. I believe that the critical path toward achieving this involves developing cross-modal synthesis and large-scale learning methods that are more generalizable and explainable. To this end, I have been focusing on the following topics:

<ul>
  <li><strong>Generative AI, Foundation Model, and Content Creation</strong>: Video generation model post-training for content creation (coming soon), Audio-to-motion generation based on DiT (under review for CVPR '26), VLM-based garment generation from a single image (under review for CVPR '26)</li>
  <li><strong>Cross-modal Synthesis and Computational Sensing</strong>: Multimodal learning for pose-to-EMG estimation (<a href="https://openreview.net/pdf/26eb537804fde5b11e5fcc37068d9f9fb6ad6fad.pdf" target="_blank">NeurIPS '25</a>), VQ-VAE-based cross-modal synthesis (<a href="https://dl.acm.org/doi/pdf/10.1145/3706598.3713465f" target="_blank">CHI '25</a>), Hand muscle electromyography inference (<a href="https://dl.acm.org/doi/pdf/10.1145/3681756.3697878" target="_blank">SA '24</a>)</li>
  <li><strong>Interative System and XR Prototype</strong>: Embodied and detached golf muscle training in AR (under review for IEEE VR '26), Aligment-based piano AR prototype (<a href="https://ieeexplore.ieee.org/abstract/document/10316487" target="_blank">ISMAR '23</a>), Alignment-based hand pose discrepancy visualization (<a href="https://dl.acm.org/doi/full/10.1145/3544549.3585705" target="_blank">CHI '23</a>)</li>
</ul>

Beyond research, I enjoy go-karting, piano playing, snowboarding, indoor bouldering, traveling, and walking friends' dogs.

Research Interests
------
Generative AI, Multimodal Learning for Content Creation, Large Foundation Model, Cross-Modal Synthesis, Human-Computer Interaction

For more info
------
More detailed info can be found in [CV](https://ruofanliu0129.github.io/Resume/cv/) and [Publications](https://ruofanliu0129.github.io/Resume/publications/). Feel free to DM me using LinkedIn! Happy to have any chats!
