---
title: "RUOFAN LIU - About Me"
description: "Self Introduction by RUOFAN LIU."
permalink: /
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Ph.D. 3rd year student (2023.10~) at the [Deparment of Computer Science](https://educ.titech.ac.jp/cs/eng/), [Institute of Science Tokyo](https://www.isct.ac.jp/en), supervised by Prof. [Hideki Koike](https://scholar.google.com/citations?hl=en&user=Ih8cJXQAAAAJ).
I am currently a visiting researcher at [Stanford Computational Imaging Lab](https://www.computationalimaging.org/), [Stanford University](https://www.stanford.edu/), supervised by Prof. [Gordon Wetzstein](https://scholar.google.com/citations?user=VOf45S0AAAAJ&hl=en), and a research assistant at [Sony Computer Science Laboratories](https://www.sonycsl.co.jp/), supervised by Prof. [Shinichi Furuya](https://scholar.google.com/citations?hl=en&user=IphDyJcAAAAJ).

I obtained my M.E. in Computer Science at the Institute of Science Tokyo with Prof. Hideki Koike, and obtained my B.E. in Computer Science and Technology at Shanghai Jiao Tong University with Prof. [Baoliang Lu](https://scholar.google.com/citations?hl=en&user=709il6EAAAAJ). 

My research lies at the intersection of <strong>computer vision, multimodal generation for content creation, foundation models, and interactive systems</strong>. My ultimate research goal is to create a next-generation deep learning framework and a generative interaction model. I believe that the critical path toward achieving this involves developing cross-modal synthesis and large-scale learning methods that are more generalizable and explainable. To this end, I have been focusing on the following topics:

<ul>
  <li><strong>Generative AI, Foundation Model, and Content Creation</strong>: Video generation model post-training for content creation (coming soon), Audio-to-motion generation based on DiT (under review for CVPR '26), VLM-based garment generation from a single image (under review for CVPR '26)</li>
  <li><strong>Cross-modal Synthesis and Computational Sensing</strong>: Multimodal learning for pose-to-EMG estimation (<a href="https://openreview.net/pdf/26eb537804fde5b11e5fcc37068d9f9fb6ad6fad.pdf" target="_blank">NeurIPS '25</a>), VQ-VAE-based cross-modal synthesis (<a href="https://dl.acm.org/doi/pdf/10.1145/3706598.3713465f" target="_blank">CHI '25</a>), Hand muscle electromyography inference (<a href="https://dl.acm.org/doi/pdf/10.1145/3681756.3697878" target="_blank">SA '24</a>)</li>
</ul>

My recent work focuses on using AI technologies to augment humans and investigates the downstream applications in embodied intelligence and skill training. By integrating visual, motor, and physiological signals such as electromyography (EMG), I aim to develop <strong><u>computational frameworks that bridge human perception, physical behavior, and interactive systems</u></strong>. My broader goal is to design intelligent tutoring systems that can perceive, model, and support human expertise in a natural and interpretable way.

Research Interests
------
Video Generation, Multimodal Learning, Mixed Reality, Human-Computer Interaction, Human Augmentation

For more info
------
More info about me can be found in my [CV](https://ruofanliu0129.github.io/Resume/cv/). Please feel free to contact me in English, Chinese, or Japanese.
